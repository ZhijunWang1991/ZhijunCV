library("Rcrawler")
library("rlist")
library("tibble")
library("lubridate")
library("stringr")
install.packages("Rcrawler",
"rlist",
"tibble",
"lubridate",
"stringr")
install.packages("Rcrawler",
"rlist",
"tibble",
"lubridate",
"stringr")
install.packages("Rcrawler")
install.packages("rlist")
install.packages("tibble")
install.packages("lubridate")
install.packages("stringr")
library("Rcrawler")
library("rlist")
library("tibble")
library("lubridate")
library("stringr")
install.packages("rlang")
install.packages("rlang")
library("Rcrawler")
library("rlist")
library("tibble")
library("lubridate")
library("stringr")
get.wiley <- function(journal=NULL){
i <- grep(journal,list$journal)
url <- list$url[i]
dat1 <- Rcrawler::ContentScraper(Url= url,
CssPatterns = c(".visitable",".meta__type",".meta__epubDate"),
PatternsName = c("title","type","date"),
ManyPerPattern = TRUE)
dat2 <- Rcrawler::ContentScraper(Url= url,
CssPatterns = c(".visitable"),
PatternsName = c("doi"),
ManyPerPattern = TRUE,astext = F)
data <- c(dat1,dat2)
data <- tibble::as_tibble(data)
data$doi <- sub("<a href=\"/doi/", "", data$doi)
data$doi <- gsub("\".*","",data$doi)
data$journal <- rep(journal, dim(data)[1])
data$date <- sub("First published: ","",data$date)
data$date <- gsub("^\\s+|\\s+$", "", data$date)#去除字符串前后空格
data$date <- sub("January","/01/",data$date)
data$date <- sub("February","/02/",data$date)
data$date <- sub("March","/03/",data$date)
data$date <- sub("April","/04/",data$date)
data$date <- sub("May","/05/",data$date)
data$date <- sub("June","/06/",data$date)
data$date <- sub("July","/07/",data$date)
data$date <- sub("August","/08/",data$date)
data$date <- sub("September","/09/",data$date)
data$date <- sub("October","/10/",data$date)
data$date <- sub("November","/11/",data$date)
data$date <- sub("December","/12/",data$date)
data$date <- gsub("\\s", "", data$date)
data$date <- lubridate::dmy(data$date)#Parsing dates and times
data$type <- stringr::str_to_title(data$type)
data$title <- stringr::str_to_title(data$title)
cat(sprintf("\n %s is OK!!! ",journal),sep = "\n")
return(data)
}
#########构造springer爬虫函数
get.springer <- function(journal= NULL){
i <- grep(journal,list$journal)
url <- list$url[i]
dat1 <- Rcrawler::ContentScraper(Url= url,
CssPatterns = c(".c-card__link",".c-meta__item"),
PatternsName = c("title","type"),
ManyPerPattern = TRUE)
dat1[["date"]] <- dat1[["type"]][c(2,4,6,8,10)]
dat1[["type"]] <- dat1[["type"]][c(1,3,5,7,9)]
dat2 <- Rcrawler::ContentScraper(Url= url,
CssPatterns = c(".c-card__link"),
PatternsName = c("doi"),
ManyPerPattern = TRUE,astext = F)
data <- c(dat1,dat2)
data <- tibble::as_tibble(data)
data$doi <- sub("<a class=\"c-card__link\" itemprop=\"url\" href=\"//link.springer.com/article/", "", data$doi)
data$doi <- sub("\".*","",data$doi)
data$journal <- rep(journal, dim(data)[1])
data$type <- sub("Content type: ","",data$type)
data$date <- sub("Published: ","",data$date)
data$date <- sub("January","/01/",data$date)
data$date <- sub("February","/02/",data$date)
data$date <- sub("March","/03/",data$date)
data$date <- sub("April","/04/",data$date)
data$date <- sub("May","/05/",data$date)
data$date <- sub("June","/06/",data$date)
data$date <- sub("July","/07/",data$date)
data$date <- sub("August","/08/",data$date)
data$date <- sub("September","/09/",data$date)
data$date <- sub("October","/10/",data$date)
data$date <- sub("November","/11/",data$date)
data$date <- sub("December","/12/",data$date)
data$date <- gsub("\\s", "", data$date)
data$date <- lubridate::dmy(data$date)#Parsing dates and times
data$type <- stringr::str_to_title(data$type)
data$title <- stringr::str_to_title(data$title)
cat(sprintf("\n %s is OK!!! ",journal),sep = "\n")
return(data)
}
#########合并爬虫函数
get.paper <- function(journal= NULL){
cat(sprintf("last Update: %s",Sys.Date()),sep = "\n")
v <- grep(journal,list$journal)
type <- list$type[v]
if(type=="wiley"){
data <- get.wiley(journal)
}
else if (type=="springer"){
data <- get.springer(journal)
}
return(data)
}
library(shiny)
library(shinydashboard)
install.packages("shinydashboard")
library(DT)
#自定义函数
rds <- tempfile(pattern=".rds")
url = 'https://gitee.com/LiuyongDing/latest_literature/raw/master/journal_list.rds'
downloader::download(url,destfile = rds, quiet = TRUE)
list <- readRDS(rds)
t <- reactive({
paste("last Update:", Sys.Date())
})
#自定义页面布局和内容
ui <- dashboardPage(skin = "green",
dashboardHeader(title = "Real-time crawl",titleWidth = 300),
dashboardSidebar(width = 300,
selectInput("journal", "Please select a journal:", choices = list$journal),
helpText("Press the button below to start crawling!!!"),
submitButton("Runing",icon = icon("table"),width='50%'),
helpText("Do not click run repeatedly!!!"),
downloadButton('downloadData', 'Download',width='50%'),
helpText("Do not click download repeatedly!!!"),
img(src="fishr.png", align = "center",height='200px',width='200px'),
helpText("Scanning the WeChat qr code for details !!!")
),
dashboardBody(
h2("A website tracking the latest literature on the fish ecology"),
textOutput("currentTime"),
textOutput("information"),
fluidRow(
box(width = 12,DTOutput("table")))
),
)
# 自定义服务器脚本
server <- shinyServer(function(input, output) {
output$currentTime <- renderText({ t() })
output$information <- renderText({ get.if(input$journal) })
output$table <- renderDT({get.paper(input$journal)})
output$downloadData <- downloadHandler(
filename = function() { paste(input$journal, '.csv', sep='') },
content = function(file) {write.csv(get.paper(input$journal), file)})
})
# Run the application
shinyApp(ui = ui, server = server)
library(shiny)
library(shinydashboard)
library(DT)
install.packages("shinydashboard")
rds <- tempfile(pattern=".rds")
url = 'https://gitee.com/LiuyongDing/latest_literature/raw/master/journal_list.rds'
downloader::download(url,destfile = rds, quiet = TRUE)
list <- readRDS(rds)
t <- reactive({
paste("last Update:", Sys.Date())
})
ui <- dashboardPage(skin = "green",
dashboardHeader(title = "Real-time crawl",titleWidth = 300),
dashboardSidebar(width = 300,
selectInput("journal", "Please select a journal:", choices = list$journal),
helpText("Press the button below to start crawling!!!"),
submitButton("Runing",icon = icon("table"),width='50%'),
helpText("Do not click run repeatedly!!!"),
downloadButton('downloadData', 'Download',width='50%'),
helpText("Do not click download repeatedly!!!"),
img(src="fishr.png", align = "center",height='200px',width='200px'),
helpText("Scanning the WeChat qr code for details !!!")
),
dashboardBody(
h2("A website tracking the latest literature on the fish ecology"),
textOutput("currentTime"),
textOutput("information"),
fluidRow(
box(width = 12,DTOutput("table")))
),
)
# 自定义服务器脚本
server <- shinyServer(function(input, output) {
output$currentTime <- renderText({ t() })
output$information <- renderText({ get.if(input$journal) })
output$table <- renderDT({get.paper(input$journal)})
output$downloadData <- downloadHandler(
filename = function() { paste(input$journal, '.csv', sep='') },
content = function(file) {write.csv(get.paper(input$journal), file)})
})
# Run the application
shinyApp(ui = ui, server = server)
get.wiley
list
view(list)
write.csv(list file = "list.csv")
write.csv(list, file = "list.csv")
install.packages("wosr")
library(wosr)
# Save your WoS API username and password in environment variables
Sys.setenv(WOS_USERNAME = "zhijun0916@gmail.com", WOS_PASSWORD = "18779189645jun!")
# Get session ID
sid <- auth()
# Save your WoS API username and password in environment variables
Sys.setenv(WOS_USERNAME = "zhijun.wang@wur.nl", WOS_PASSWORD = "Lipton2020!")
# Get session ID
sid <- auth()
unlink('C:/Users/wang298/OneDrive - WageningenUR/Banana/paper3/Foods/Foods_banana/Foods_banana_cache', recursive = TRUE)
knit_with_parameters('C:/Users/wang298/OneDrive - WageningenUR/Banana/paper3/Foods/Foods_banana/Foods_banana.Rmd')
library(caret)
install.packages("caret")
# NOT RUN {
# }
# NOT RUN {
data(mdrr)
set.seed(1)
inTrain <- sample(seq(along = mdrrClass), 450)
nzv <- nearZeroVar(mdrrDescr)
filteredDescr <- mdrrDescr[, -nzv]
training <- filteredDescr[inTrain,]
test <- filteredDescr[-inTrain,]
trainMDRR <- mdrrClass[inTrain]
testMDRR <- mdrrClass[-inTrain]
preProcValues <- preProcess(training)
trainDescr <- predict(preProcValues, training)
testDescr <- predict(preProcValues, test)
useBayes   <- plsda(trainDescr, trainMDRR, ncomp = 5,
probMethod = "Bayes")
useSoftmax <- plsda(trainDescr, trainMDRR, ncomp = 5)
confusionMatrix(predict(useBayes, testDescr),
testMDRR)
confusionMatrix(predict(useSoftmax, testDescr),
testMDRR)
histogram(~predict(useBayes, testDescr, type = "prob")[,"Active",]
| testMDRR, xlab = "Active Prob", xlim = c(-.1,1.1))
histogram(~predict(useSoftmax, testDescr, type = "prob")[,"Active",]
| testMDRR, xlab = "Active Prob", xlim = c(-.1,1.1))
## different sized objects are returned
length(predict(useBayes, testDescr))
dim(predict(useBayes, testDescr, ncomp = 1:3))
dim(predict(useBayes, testDescr, type = "prob"))
dim(predict(useBayes, testDescr, type = "prob", ncomp = 1:3))
## Using spls:
## (As of 11/09, the spls package now has a similar function with
## the same mane. To avoid conflicts, use caret:::splsda to
## get this version)
splsFit <- caret:::splsda(trainDescr, trainMDRR,
K = 5, eta = .9,
probMethod = "Bayes")
confusionMatrix(caret:::predict.splsda(splsFit, testDescr),
testMDRR)
# }
# NOT RUN {
# }
# NOT RUN {
# }
# NOT RUN {
data(mdrr)
# NOT RUN {
# }
# NOT RUN {
library(caret)
data(mdrr)
set.seed(1)
inTrain <- sample(seq(along = mdrrClass), 450)
nzv <- nearZeroVar(mdrrDescr)
filteredDescr <- mdrrDescr[, -nzv]
training <- filteredDescr[inTrain,]
test <- filteredDescr[-inTrain,]
trainMDRR <- mdrrClass[inTrain]
testMDRR <- mdrrClass[-inTrain]
preProcValues <- preProcess(training)
trainDescr <- predict(preProcValues, training)
testDescr <- predict(preProcValues, test)
useBayes   <- plsda(trainDescr, trainMDRR, ncomp = 5,
probMethod = "Bayes")
useSoftmax <- plsda(trainDescr, trainMDRR, ncomp = 5)
confusionMatrix(predict(useBayes, testDescr),
testMDRR)
confusionMatrix(predict(useSoftmax, testDescr),
testMDRR)
histogram(~predict(useBayes, testDescr, type = "prob")[,"Active",]
| testMDRR, xlab = "Active Prob", xlim = c(-.1,1.1))
histogram(~predict(useSoftmax, testDescr, type = "prob")[,"Active",]
| testMDRR, xlab = "Active Prob", xlim = c(-.1,1.1))
## different sized objects are returned
length(predict(useBayes, testDescr))
dim(predict(useBayes, testDescr, ncomp = 1:3))
dim(predict(useBayes, testDescr, type = "prob"))
dim(predict(useBayes, testDescr, type = "prob", ncomp = 1:3))
splsFit <- caret:::splsda(trainDescr, trainMDRR,
K = 5, eta = .9,
probMethod = "Bayes")
confusionMatrix(caret:::predict.splsda(splsFit, testDescr),
testMDRR)
install.packages(spls)
install.packages("spls")
splsFit <- caret:::splsda(trainDescr, trainMDRR,
K = 5, eta = .9,
probMethod = "Bayes")
confusionMatrix(caret:::predict.splsda(splsFit, testDescr),
testMDRR)
nzv <- nearZeroVar(mdrrDescr)
filteredDescr <- mdrrDescr[, -nzv]
training <- filteredDescr[inTrain,]
test <- filteredDescr[-inTrain,]
trainMDRR <- mdrrClass[inTrain]
testMDRR <- mdrrClass[-inTrain]
preProcValues <- preProcess(training)
trainDescr <- predict(preProcValues, training)
testDescr <- predict(preProcValues, test)
useBayes   <- plsda(trainDescr, trainMDRR, ncomp = 5,
probMethod = "Bayes")
useSoftmax <- plsda(trainDescr, trainMDRR, ncomp = 5)
confusionMatrix(predict(useBayes, testDescr),
testMDRR)
install.packages("rnaturalearthhires", repos = "http://packages.ropensci.org", type = "source")
library(rnaturalearth)
library(sf)
library(raster)
library(tidyverse)
library(RColorBrewer)
library(sp)
update.packages(sp)
update.packages('sp')
install.packages("dplyr")
install.packages("sp")
library(raster)
world <- ne_countries(scale = 50) #world map with 50m resolution
#packages
library(rnaturalearth)
library(sf)
library(raster)
library(tidyverse)
library(RColorBrewer)
world <- ne_countries(scale = 50) #world map with 50m resolution
plot(world)
iceland <- ne_countries(scale = 10, country = "Iceland", returnclass = "sf")
install.packages("rgeos")
iceland <- ne_countries(scale = 10, country = "Iceland", returnclass = "sf")
iceland
plot(iceland)
China <- ne_countries(scale = 10, country = "China", returnclass = "sf")
plot(China)
Taiwan <- ne_countries(scale = 10, country = "Taiwan", returnclass = "sf")
plot(Taiwan)
Colombia <- ne_countries(scale = 10, country = "Colombia", returnclass = "sf")
plot(Colombia)
your_points <- st_as_sf(tibble(x = -21.933333, y = 64.133333) , coords = c("x", "y"), crs = 4326) %>%
st_transform(3055)
dist <- st_distance(iceland, your_points)
iceland <- st_transform(iceland, 3055)
dist <- st_distance(iceland, your_points)
dist
grid <- st_make_grid(iceland, cellsize = 5000, what = "centers")
grid <- st_intersection(grid, iceland)
plot(grid)
your_points <- st_as_sf(tibble(x = -21.933333, y = 64.133333) , coords = c("x", "y"), crs = 4326) %>%
st_transform(3055)
dist <- st_distance(iceland, your_points)
head(dist)
plot(iceland)
df <- data.frame(dist = as.vector(dist)/1000,
st_coordinates(grid))
#structure
str(df)
col_dist <- brewer.pal(11, "RdGy")
ggplot(df, aes(X, Y, fill = dist))+ #variables
geom_tile()+ #geometry
scale_fill_gradientn(colours = rev(col_dist))+ #colors for plotting the distance
labs(fill = "Distance (km)")+ #legend name
theme_void()+ #map theme
theme(legend.position = "bottom") #legend position
your_points <- st_as_sf(tibble(x = -66.044051, y = -15.999817) , coords = c("x", "y"), crs = 4326) %>%
st_transform(3055)
dist <- st_distance(iceland, your_points)
head(dist)
df <- data.frame(dist = as.vector(dist)/1000,
st_coordinates(grid))
#structure
str(df)
col_dist <- brewer.pal(11, "RdGy")
ggplot(df, aes(X, Y, fill = dist))+ #variables
geom_tile()+ #geometry
scale_fill_gradientn(colours = rev(col_dist))+ #colors for plotting the distance
labs(fill = "Distance (km)")+ #legend name
theme_void()+ #map theme
theme(legend.position = "bottom") #legend position
world <- ne_countries(scale = 50) #world map with 50m resolution
plot(world)
Colombia <- ne_countries(scale = 10, country = "Colombia", returnclass = "sf")
plot(Colombia)
Colombia <- st_transform(iceland, 3055)
Colombia <- st_transform(Colombia, 3055)
Colombia
grid <- st_make_grid(Colombia, cellsize = 5000, what = "centers")
grid <- st_intersection(Colombia, iceland)
plot(grid)
CO1 <- st_as_sf(tibble(x = 3.950806,y = -76.543000) , coords = c("x", "y"), crs = 4326) %>%
st_transform(3055)
dist_CO1 <- st_distance(Colombia, CO1)
head(dist_CO1)
dist  <- st_distance(Colombia, CO1)
head(dist)
df <- data.frame(dist = as.vector(dist)/1000,
st_coordinates(grid))
#structure
str(df)
col_dist <- brewer.pal(11, "RdGy")
ggplot(df, aes(X, Y, fill = dist))+ #variables
geom_tile()+ #geometry
scale_fill_gradientn(colours = rev(col_dist))+ #colors for plotting the distance
labs(fill = "Distance (km)")+ #legend name
theme_void()+ #map theme
theme(legend.position = "bottom") #legend position
CO1 <- st_as_sf(tibble(x = -76.543000,y = 3.950806) , coords = c("x", "y"), crs = 4326) %>%
st_transform(3055)
dist  <- st_distance(Colombia, CO1)
head(dist)
df <- data.frame(dist = as.vector(dist)/1000,
st_coordinates(grid))
#structure
str(df)
col_dist <- brewer.pal(11, "RdGy")
ggplot(df, aes(X, Y, fill = dist))+ #variables
geom_tile()+ #geometry
scale_fill_gradientn(colours = rev(col_dist))+ #colors for plotting the distance
labs(fill = "Distance (km)")+ #legend name
theme_void()+ #map theme
theme(legend.position = "bottom") #legend position
CO1 <- st_as_sf(tibble(x = -76.543000,y = 3.950806) , coords = c("x", "y"), crs = 4326) %>%
st_transform(3055)
dist  <- st_distance(Colombia, CO1)
head(dist)
CO1 <- st_as_sf(tibble(x = 3.950806,y = -76.543000) , coords = c("x", "y"), crs = 4326) %>%
st_transform(3055)
dist  <- st_distance(Colombia, CO1)
head(dist)
df <- data.frame(dist = as.vector(dist)/1000,
st_coordinates(grid))
#structure
str(df)
col_dist <- brewer.pal(11, "RdGy")
ggplot(df, aes(X, Y, fill = dist))+ #variables
geom_tile()+ #geometry
scale_fill_gradientn(colours = rev(col_dist))+ #colors for plotting the distance
labs(fill = "Distance (km)")+ #legend name
theme_void()+ #map theme
## Not run:
# # load iris dataset
data(iris)
library(DiscriMiner)
install.packages("DiscriMiner")
library(DiscriMiner)
# # PLS discriminant analysis specifying number of components = 2
my_pls1 = plsDA(iris[,1:4], iris$Species, autosel=FALSE, comps=2)
my_pls1$confusion
my_pls1$error_rate
# # plot circle of correlations
plot(my_pls1)
# # PLS discriminant analysis with automatic selection of components
my_pls2 = plsDA(iris[,1:4], iris$Species, autosel=TRUE)
my_pls2$confusion
my_pls2$error_rate
# # linear discriminant analysis with learn-test validation
learning = c(1:40, 51:90, 101:140)
testing = c(41:50, 91:100, 141:150)
my_pls3 = plsDA(iris[,1:4], iris$Species, validation="learntest",
learn=learning, test=testing)
my_pls3$confusion
my_pls3$error_rate
setwd('C:\blogdown')
setwd("C:/blogdown/content/")
setwd("C:\blogdown\content\")
setwd("C:\blogdown\")
setwd("C:\blogdown\butter")
setwd("C:\blogdown\butter\")
libraruy
blogdown:::new_post_addin()
library(DiscriMiner)
getwd()
setwd("C:/blogdown/content")
setwd("C:/blogdown/butter")
blogdown:::new_post_addin()
library(dslabs)
library(dplyr)
library(lubridate)
data(reported_heights)
dat <- mutate(reported_heights, date_time = ymd_hms(time_stamp)) %>%
filter(date_time >= make_date(2016, 01, 25) & date_time < make_date(2016, 02, 1)) %>%
mutate(type = ifelse(day(date_time) == 25 & hour(date_time) == 8 & between(minute(date_time), 15, 30), "inclass","online")) %>%
select(sex, type)
y <- factor(dat$sex, c("Female", "Male"))
x <- dat$type
library(caret)
blogdown:::serve_site()
